{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld import *\n",
    "% matplotlib inline\n",
    "\n",
    "# create the gridworld as a specific MDP\n",
    "gridworld=GridMDP([[-0.04,-0.04,-0.04,1],[-0.04,None, -0.04, -1], [-0.04, -0.04, -0.04, -0.04]], terminals=[(3,2), (3,1)], gamma=1.)\n",
    "\n",
    "example_pi = {(0,0): (0,1), (0,1): (0,1), (0,2): (1,0), (1,0): (1,0), (1,2): (1,0), (2,0): (0,1), (2,1): (0,1), (2,2): (1,0), (3,0):(-1,0), (3,1): None, (3,2):None}\n",
    "\n",
    "example_q = {\t((0,0),(-1,0)): 0.1,((0,0),(1,0)): 0.4,((0,0),(0,1)): 0.3,((0,0),(0,-1)): 0.2,\n",
    "\t\t((0,1),(-1,0)): 0.0,((0,1),(1,0)): 0.2,((0,1),(0,1)): 0.5,((0,1),(0,-1)): 0.1,\n",
    "\t\t((0,2),(-1,0)): 0.1,((0,2),(1,0)): 0.6,((0,2),(0,1)): 0.1,((0,2),(0,-1)): 0.0,\n",
    "\t\t((1,0),(-1,0)): 0.4,((1,0),(1,0)): 0.2,((1,0),(0,1)): 0.1,((1,0),(0,-1)): 0.2,\n",
    "\t\t((1,2),(-1,0)): 0.2,((1,2),(1,0)): 0.7,((1,2),(0,1)): 0.4,((1,2),(0,-1)): 0.3,\n",
    "\t\t((2,0),(-1,0)): 0.3,((2,0),(1,0)): -0.2,((2,0),(0,1)): 0.3,((2,0),(0,-1)): 0.1,\n",
    "\t\t((2,1),(-1,0)): 0.1,((2,1),(1,0)): -0.4,((2,1),(0,1)): 0.3,((2,1),(0,-1)): -0.1,\n",
    "\t\t((2,2),(-1,0)): 0.3,((2,2),(1,0)): 0.8,((2,2),(0,1)): 0.3,((2,2),(0,-1)): 0.3,\n",
    "\t\t((3,0),(-1,0)): 0.2,((3,0),(1,0)): -0.3,((3,0),(0,1)): -0.6,((3,0),(0,-1)): 0.2,\n",
    "\t\t((3,1), None): -0.8, ((3,2),None):0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1)\tComplete the function passive_td, which implements temporal-\t\n",
    "\tdifference learning for fixed policies. The function takes as input a \t\t\n",
    "\tpolicy, the MDP and returns as output the value function. \t\n",
    "\"\"\"\n",
    "\n",
    "def passive_td(pi, mdp, alpha = 0.05, n_trials = 500):\n",
    "\tV = dict([(s, 0) for s in mdp.states]) # initialize value function\n",
    "\tR, gamma = mdp.R, mdp.gamma\n",
    "\tfor i in range(n_trials):\n",
    "\t\tstate = (0,0)\n",
    "\t\twhile True:\t\t\n",
    "\t\t\tnewstate = mdp.new_state(state, pi[state])\n",
    "\t\t\traise NotImplementedError # implement TD update here\n",
    "\t\t\tif newstate in mdp.terminals:\n",
    "\t\t\t\traise NotImplementedError # final TD update\n",
    "\t\t\t\tbreak\n",
    "\t\t\tstate = newstate\n",
    "\treturn V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V=passive_td(example_pi, gridworld)\n",
    "gridworld.v_plot(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2)\tImplement Q-learning in the function below and complete the function best_policy_q which \n",
    "\tcomputes the best policy given a q-function. Compare the q-function you get with the q-function\n",
    "\tcomputed by value_iteration_q.\t\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" function argmax is needed for choosing the optimal action \"\"\"\n",
    "def argmax(seq, fn):\n",
    "    best = seq[0]; best_score = fn(best)\n",
    "    for x in seq:\n",
    "        x_score = fn(x)\n",
    "        if x_score > best_score:\n",
    "            best, best_score = x, x_score\n",
    "    return best\n",
    "\n",
    "\n",
    "def q_learning(mdp, alpha = 0.1, n_trials=500):\n",
    "\t# initialize Q\n",
    "\tQ=dict([ ((s,a),0) for s in mdp.states for a in mdp.actions(s)])\n",
    "\tR, gamma = mdp.R, mdp.gamma\t\n",
    "\tfor i in range(n_trials):\n",
    "\t\tstate = (0,0)\n",
    "\t\twhile True:\t\n",
    "\t\t\traise NotImplementedError # implement action selection, state update and TD update\n",
    "            \n",
    "            \n",
    "\t\t\tif newstate in mdp.terminals:\n",
    "\t\t\t\tQ[newstate, None] = Q[newstate, None]  + alpha*( R(newstate) -Q[newstate, None]) \n",
    "\t\t\t\tbreak\n",
    "\t\t\tstate = newstate\n",
    "\treturn Q\n",
    "\n",
    "\n",
    "def best_policy_q(mdp, Q):\n",
    "    \"\"\"Given an MDP and a q function, determine the best policy,\n",
    "    as a mapping from state to action. \"\"\"\n",
    "    pi = {}\n",
    "    raise NotImplementedError # assign optimal policy by looping through states\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "def value_iteration_q(mdp, epsilon=0.0001):\n",
    "    \"Computing the q-function by value iteration. This assumes the transition model is known\"\n",
    "    Q1=dict([ ((s,a),0) for s in mdp.states for a in mdp.actions(s)]) # initialize value function\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        Q = Q1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "\t    for a in mdp.actions(s):\t\n",
    "            \tQ1[s,a] = R(s) + gamma * sum([p * max([Q[s1,a1] for a1 in mdp.actions(s1)])  for (p, s1) in T(s, a)])\n",
    "            \tdelta = max(delta, abs(Q1[s,a] - Q[s,a]))\n",
    "        if delta < epsilon:\n",
    "             return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = q_learning(gridworld)\n",
    "Qval = value_iteration_q(gridworld)\n",
    "gridworld.q_plot(Q)\n",
    "gridworld.q_plot(Qval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "3)\tModify the function Q-learning so that it implements an epsilon-greedy agent.\n",
    "\"\"\"\n",
    "\n",
    "def epsilon_q_learning(mdp, epsilon, alpha = 0.1, n_trials=500):\n",
    "\t# initialize Q\n",
    "\tQ=dict([ ((s,a),0) for s in mdp.states for a in mdp.actions(s)])\n",
    "\tR, gamma = mdp.R, mdp.gamma\t\n",
    "\tfor i in range(n_trials):\n",
    "        raise NotImplementedError   # implement the epsilon-greedy q-learner\n",
    "                                    # use the function random.choice(mdp.actions(state)) for random action selection\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\treturn Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Qeps = epsilon_q_learning(gridworld, 0.5)\n",
    "gridworld.q_plot(Qeps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
